<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Keras Practical Cheat Sheet</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; background: #f8f9fb; color: #1b1f24; }
    h1, h2, h3 { color: #222; }
    pre { background: #e6e8eb; padding: 12px; border-radius: 8px; overflow-x: auto; }
    code { font-family: monospace; }
    .example { margin-bottom: 40px; }
    .note { background: #eef4ff; padding: 8px; border-left: 4px solid #0b6cff; margin: 8px 0; border-radius: 6px; }
  </style>
</head>
<body>
  <h1>Keras Practical Cheat Sheet</h1>
  <p>Step-by-step examples to learn Keras Sequential models with Dense layers.</p>

  <div class="example">
    <h2>1. Import libraries</h2>
    <pre><code>from tensorflow import keras
import numpy as np</code></pre>
    <p>We always need TensorFlow/Keras and NumPy for handling data.</p>
  </div>

  <div class="example">
    <h2>2. Training data basics</h2>
    <pre><code>x = np.array([[1],[2],[3],[4],[5]], dtype=float)
y = np.array([2,4,6,8,10], dtype=float)  # y = 2x
print(x.shape, y.shape)  # (5, 1) (5,)
</code></pre>
    <p><b>Inputs</b> must be 2D arrays (samples × features). <b>Outputs</b> can be 1D for simple regression.</p>
    <p>All a neural network does is take your inputs and outputs (x,y) as training data. It will then try to figure out what the relationshiop is between them.</p>
    <p>It's just math. In this example, (X = 1, Y=2) or (X=2, Y=4). The neural network needs to figure out that the pattern is y=2x.</p>
    <strong>it's just a game of numbers</strong>
  </div>

  <div class="example">
    <h2>3. Build a simple model</h2>
    <pre><code>model = keras.Sequential([
    keras.Input(shape=(1,)),  # 1 feature per sample
    keras.layers.Dense(1)      # 1 output neuron
])</code></pre>
    <p>Sequential stacks layers in order. Dense(1) is enough for linear regression.</p>
  </div>

  <div class="example">
    <h2>4. Compile the model</h2>
    <pre><code>optimizer = keras.optimizers.SGD(learning_rate=0.001) #optional. you don't need
model.compile(optimizer=optimizer, loss="mean_squared_error")</code></pre>
    <p>Use <code>SGD</code> for simple regression and <code>mean_squared_error</code> as loss.</p>
    <p>You do not need optimizers. model.compile(loss="mean_squared_error") works just fine. I assume you know what a loss method is</p>
  </div>

  <div class="example">
    <h2>5. Train the model</h2>
    <pre><code>model.fit(x, y, epochs=500, verbose=0)</code></pre>
    <p><code>epochs</code> = number of passes through the dataset. <code>verbose=0</code> suppresses output.</p>
  </div>

  <div class="example">
    <h2>6. Make predictions</h2>
    <pre><code>x_new = np.array([[6],[7]])
print(model.predict(x_new))  # ~ [[12], [14]]</code></pre>
  </div>

  <div class="example">
    <h2>7. Simple variations</h2>
    <h3>a) y = x + 1</h3>
    <pre><code>x = np.array([[1],[2],[3],[4],[5]], dtype=float)
y = np.array([2,3,4,5,6], dtype=float)

model = keras.Sequential([
    keras.Input(shape=(1,)),
    keras.layers.Dense(1)
])
model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001),
              loss="mean_squared_error")
model.fit(x, y, epochs=500, verbose=0)
print(model.predict(np.array([[10]])))  # ~11</code></pre>

    <h3>b) Nonlinear: y = x²</h3>
    <pre><code>x = np.array([[1],[2],[3],[4],[5]], dtype=float)
y = np.array([1,4,9,16,25], dtype=float)

model = keras.Sequential([
    keras.Input(shape=(1,)),
    keras.layers.Dense(10, activation="relu"),  # hidden layer needed
    keras.layers.Dense(1)
])
model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001),
              loss="mean_squared_error")
model.fit(x, y, epochs=1000, verbose=0)
print(model.predict(np.array([[6]])))  # ~36</code></pre>
    <div class="note">Hidden layers are only necessary for nonlinear relationships. Linear regression does not need them.</div>
  </div>

  <div class="example">
    <h2>8. Notes & Tips</h2>
    <ul>
      <li>Keep inputs as NumPy arrays, not Python lists.</li>
      <li>Use <code>learning_rate</code> inside the optimizer, not in compile.</li>
      <li>1 output neuron is enough for simple regression.</li>
      <li>Increase hidden neurons only for nonlinear mappings.</li>
    </ul>
  </div>
</body>
</html>
